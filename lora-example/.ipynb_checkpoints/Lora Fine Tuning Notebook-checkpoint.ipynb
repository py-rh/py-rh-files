{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78f44e55-4860-433b-b56e-cc9fcd4b9d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2024-08-09 19:49:00.718467 | Loaded Runhouse config from /Users/paulyang/.rh/config.yaml\n",
      "/opt/anaconda3/envs/tempenv/lib/python3.11/site-packages/asyncssh/crypto/cipher.py:29: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from this module in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n",
      "/opt/anaconda3/envs/tempenv/lib/python3.11/site-packages/asyncssh/crypto/cipher.py:30: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import TripleDES\n",
      "/opt/anaconda3/envs/tempenv/lib/python3.11/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "/opt/anaconda3/envs/tempenv/lib/python3.11/site-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n",
      "INFO | 2024-08-09 19:49:02.760484 | PyTorch version 2.4.0 available.\n"
     ]
    }
   ],
   "source": [
    "#!pip install \"runhouse[aws]\" torch datasets transformers peft trl\n",
    "import runhouse as rh \n",
    "import os\n",
    "\n",
    "#os.chdir(\"/dir/mydir\")\n",
    "from LoraFineTuner import FineTuner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ff4ea8-4663-47eb-8293-79a6b11e9324",
   "metadata": {},
   "source": [
    "## Connect to cluster\n",
    "**If you would like to run your own cluster instead:**\n",
    "```python\n",
    "# Run `sky check` to confirm AWS credentials are setup\n",
    "cluster = rh.cluster(\n",
    "    name=\"rh-a10x\",\n",
    "    instance_type=\"A10G:1\",\n",
    "    memory=\"32+\",\n",
    "    provider=\"aws\",\n",
    ").up_if_not()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e15ea72f-d0dd-460a-bcb3-bf3c479a0b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = rh.cluster(name=\"/rh-alpha-testers/jamesb\")\n",
    "\n",
    "# You will need a HF_TOKEN as an env variable to download the pretrained model in this example\n",
    "# Reqs will be installed by Runhouse on remote\n",
    "# We can also show you how to launch with a Docker container / conda env \n",
    "env = rh.env(\n",
    "    name=\"ft_env\",\n",
    "    reqs=[\n",
    "        \"torch\",\n",
    "        \"tensorboard\",\n",
    "        \"scipy\",\n",
    "        \"peft==0.4.0\",\n",
    "        \"bitsandbytes==0.40.2\",\n",
    "        \"transformers==4.31.0\",\n",
    "        \"trl==0.4.7\",\n",
    "        \"accelerate\",\n",
    "    ],\n",
    "    secrets=[\"huggingface\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8e0c45-16b0-43a0-91e6-d3bf64000598",
   "metadata": {},
   "source": [
    "## Send fine tuner to remote and instantiate / Get already instantiated remote instance\n",
    "* There is a locally defined LoraFineTuner class\n",
    "* Runhouse will send this class to remote compute\n",
    "* Then locally we create an *instance* of this remote class, which we name `rh_finetuner` or anything else\n",
    "* We call this remote-instance from local as if it were normal/local, and can access it by name from any Python session connected to the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f06aeffd-e799-42c3-badf-41a43fe84378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2024-08-09 14:40:31.580499 | Running forwarding command: ssh -T -L 32300:localhost:32300 -i ~/.ssh/sky-key -o Port=22 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o ExitOnForwardFailure=yes -o ServerAliveInterval=5 -o ServerAliveCountMax=3 -o ConnectTimeout=30s -o ForwardAgent=yes -o ControlMaster=auto -o ControlPath=/tmp/skypilot_ssh_6ba66db0/801cb87b25/%C -o ControlPersist=300s ubuntu@3.95.222.246\n"
     ]
    }
   ],
   "source": [
    "fine_tuner_remote_name = \"rh_finetuner\" ## This is the name of the *instance* of the remote class, not the remote class\n",
    "\n",
    "# We check if we have already created a \"rh_finetuner\" on the remote which is an *instance* of the remote fine tuner class\n",
    "fine_tuner_remote = cluster.get(fine_tuner_remote_name, default=None, remote=True)\n",
    "\n",
    "# If we have not, then we will send the local class to remote, and create an instance of it named \"rh_finetuner\"\n",
    "# If you disconnect locally after calling tune, you can simply reconnect to the remote object using this block from another local session\n",
    "if fine_tuner_remote is None:\n",
    "    fine_tuner = rh.module(FineTuner).to(\n",
    "        cluster, env=env, name=\"llama3-medical-model\"\n",
    "    )\n",
    "    fine_tuner_remote = fine_tuner(name=fine_tuner_remote_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3e9e12b-4120-4246-810c-d7c9d1924308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2024-08-09 14:49:08.678207 | Calling rh_finetuner.tune\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mft_env env: Calling method tune on module rh_finetuner\n",
      "\u001b[0m\u001b[36mft_env env: Calling method new_model_exists on module rh_finetuner\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2024-08-09 14:49:09.238983 | Time to call rh_finetuner.tune: 0.56 seconds\n"
     ]
    }
   ],
   "source": [
    "## Once we have accessed the remote class, we can call against it as if it were a local object \n",
    "fine_tuner_remote.tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1be00fae-9205-4275-9e16-f750991e6f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2024-08-09 14:41:07.557128 | Calling rh_finetuner.generate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mft_env env: Calling method generate on module rh_finetuner\n",
      "\u001b[0m\u001b[36mft_env env: Calling method load_pipeline on module rh_finetuner\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2024-08-09 14:41:08.122288 | Time to call rh_finetuner.generate: 0.57 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>system<|end_header_id|> Answer the question truthfully, you are a medical professional.<|eot_id|><|start_header_id|>user<|end_header_id|> This is the question: What's the best treatment for sunburn?<|eot_id|><|start_header_id|>assistant<|end_header_id|> The best treatment for sunburn is to prevent it.\n"
     ]
    }
   ],
   "source": [
    "# Once the fine tuner is complete, we can query against it \n",
    "query = \"What's the best treatment for sunburn?\"\n",
    "generated_text = fine_tuner_remote.generate(query, max_length = 1000)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc3c5d0-7980-4e3b-a192-68bbe6214eab",
   "metadata": {},
   "source": [
    "## I can reconnect this remote instance of the fine tuner even after my local session disconnects, or from another session\n",
    "If I connect to the cluster, and get the object by name, I can call against it even if my local session ends\n",
    "\n",
    "You can run the below code at any point, including while training is running, from anywhere (like another notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f27d4764-e797-4610-b886-48b2820fe7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2024-08-09 14:58:13.149741 | SSH tunnel on to server's port 32300 via server's ssh port 22 already created with the cluster.\n",
      "INFO | 2024-08-09 14:58:13.214864 | Calling rh_finetuner.get_training_status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "\u001b[36mrh-a10x\u001b[0m\n",
      "-------\n",
      "\u001b[36mft_env env: Calling method get_training_status on module rh_finetuner\n",
      "\u001b[0m\u001b[36mft_env env: Calling method new_model_exists on module rh_finetuner\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2024-08-09 14:58:13.738335 | Time to call rh_finetuner.get_training_status: 0.52 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_model_loaded': False, 'tokenizer_loaded': True, 'fine_tuned_model_loaded': True, 'pipeline_loaded': True, 'training_completed': True}\n"
     ]
    }
   ],
   "source": [
    "import runhouse as rh \n",
    "\n",
    "# We check if we have already created a \"rh_finetuner\" on the remote which is an *instance* of the remote fine tuner class\n",
    "cluster = rh.cluster(name=\"/paul/rh-a10x\")\n",
    "fine_tuner_remote_name = \"rh_finetuner\"\n",
    "fine_tuner_remote = cluster.get(fine_tuner_remote_name, default=None, remote=True)\n",
    "\n",
    "# Check what the training status is on remote\n",
    "if fine_tuner_remote is not None:\n",
    "    print(fine_tuner_remote.get_training_status())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60798e4-0a38-4c51-a758-f7196762a21b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
